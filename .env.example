# Torbo Base — Environment Configuration
# Copy this to .env and fill in your API keys
# cp .env.example .env

# ─── Server ───────────────────────────────────────────────
TORBO_PORT=4200
# Access level: 0=OFF, 1=CHAT, 2=READ, 3=WRITE, 4=EXEC, 5=FULL
TORBO_ACCESS_LEVEL=1
# Server auth token (auto-generated if empty)
TORBO_TOKEN=

# ─── LLM Providers (at least one required) ────────────────
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
XAI_API_KEY=
GOOGLE_API_KEY=

# ─── Voice (optional) ─────────────────────────────────────
ELEVENLABS_API_KEY=

# ─── Local Models ─────────────────────────────────────────
# Ollama URL — defaults to host.docker.internal:11434
# If running Ollama on the Docker host, this should work as-is.
# If Ollama is in another container, use its container name.
OLLAMA_HOST=http://host.docker.internal:11434

# ─── Messaging Bridges (optional) ─────────────────────────
TELEGRAM_BOT_TOKEN=
DISCORD_BOT_TOKEN=
SLACK_BOT_TOKEN=
SIGNAL_PHONE=
